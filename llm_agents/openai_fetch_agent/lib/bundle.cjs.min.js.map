{"version":3,"file":"bundle.cjs.min.js","sources":["../../llm_utils/lib/index.js","../src/openai_fetch_agent.ts"],"sourcesContent":["\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.llmMetaDataFirstTokenTime = exports.llmMetaDataEndTime = exports.initLLMMetaData = exports.convertMeta = exports.getMessages = exports.getMergeValue = exports.flatString = void 0;\nconst flatString = (input) => {\n    return Array.isArray(input) ? input.filter((a) => a).join(\"\\n\") : (input ?? \"\");\n};\nexports.flatString = flatString;\nconst getMergeValue = (namedInputs, params, key, values) => {\n    const inputValue = namedInputs[key];\n    const paramsValue = params[key];\n    return inputValue || paramsValue ? [(0, exports.flatString)(inputValue), (0, exports.flatString)(paramsValue)].filter((a) => a).join(\"\\n\") : (0, exports.flatString)(values);\n};\nexports.getMergeValue = getMergeValue;\nconst getMessages = (systemPrompt, messages) => {\n    const messagesCopy = [...(systemPrompt ? [{ role: \"system\", content: systemPrompt }] : []), ...(messages ?? [])];\n    return messagesCopy;\n};\nexports.getMessages = getMessages;\n//\nconst convertMeta = (llmMetaData) => {\n    const { start, firstToken, end } = llmMetaData.timing;\n    const latencyToFirstToken = firstToken ? firstToken - start : undefined;\n    const duration = firstToken ? end - firstToken : undefined;\n    const totalElapsed = end - start;\n    return {\n        timing: {\n            start: new Date(start).toISOString(),\n            firstToken: firstToken ? new Date(firstToken).toISOString() : undefined,\n            end: new Date(end).toISOString(),\n            latencyToFirstToken,\n            duration,\n            totalElapsed,\n        },\n    };\n};\nexports.convertMeta = convertMeta;\nconst initLLMMetaData = () => {\n    const llmMetaData = { timing: { start: Date.now(), end: 0, totalElapsed: 0 } };\n    return llmMetaData;\n};\nexports.initLLMMetaData = initLLMMetaData;\nconst llmMetaDataEndTime = (llmMetaData) => {\n    llmMetaData.timing.end = Date.now();\n};\nexports.llmMetaDataEndTime = llmMetaDataEndTime;\nconst llmMetaDataFirstTokenTime = (llmMetaData) => {\n    if (llmMetaData.timing.firstToken === undefined) {\n        llmMetaData.timing.firstToken = Date.now();\n    }\n};\nexports.llmMetaDataFirstTokenTime = llmMetaDataFirstTokenTime;\n","import OpenAI from \"openai\";\nimport { AgentFunction, AgentFunctionInfo, sleep } from \"graphai\";\nimport { GraphAILLMInputBase, getMergeValue, getMessages } from \"@graphai/llm_utils\";\n\ntype OpenAIInputs = {\n  model?: string;\n  images?: string[];\n  tools?: OpenAI.ChatCompletionTool[];\n  tool_choice?: OpenAI.ChatCompletionToolChoiceOption;\n  max_tokens?: number;\n  verbose?: boolean;\n  temperature?: number;\n  messages?: Array<OpenAI.ChatCompletionMessageParam>;\n  response_format?: any;\n} & GraphAILLMInputBase;\n\ntype OpenAIConfig = {\n  baseURL?: string;\n  apiKey?: string;\n  stream?: boolean;\n};\n\ntype OpenAIParams = OpenAIInputs & OpenAIConfig;\n\nconst convertOpenAIChatCompletion = (response: OpenAI.ChatCompletion, messages: OpenAI.ChatCompletionMessageParam[]) => {\n  const message = response?.choices[0] && response?.choices[0].message ? response?.choices[0].message : null;\n  const text = message && message.content ? message.content : null;\n\n  const functionResponse = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0] : null;\n  // const functionId = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0]?.id : null;\n\n  const tool = functionResponse\n    ? {\n        id: functionResponse.id,\n        name: functionResponse?.function?.name,\n        arguments: (() => {\n          try {\n            return JSON.parse(functionResponse?.function?.arguments);\n          } catch (__e) {\n            return undefined;\n          }\n        })(),\n      }\n    : undefined;\n\n  if (message) {\n    messages.push(message);\n  }\n  return {\n    ...response,\n    text,\n    tool,\n    message,\n    messages,\n  };\n};\n\nexport const openAIFetchAgent: AgentFunction<OpenAIParams, Record<string, any> | string, OpenAIInputs, OpenAIConfig> = async ({\n  filterParams,\n  params,\n  namedInputs,\n  config,\n}) => {\n  const { verbose, system, images, temperature, tools, tool_choice, max_tokens, prompt, messages, response_format } = {\n    ...params,\n    ...namedInputs,\n  };\n\n  const { apiKey, stream, baseURL } = {\n    ...params,\n    ...(config || {}),\n  };\n\n  const userPrompt = getMergeValue(namedInputs, params, \"mergeablePrompts\", prompt);\n  const systemPrompt = getMergeValue(namedInputs, params, \"mergeableSystem\", system);\n\n  const messagesCopy = getMessages<OpenAI.ChatCompletionMessageParam>(systemPrompt, messages);\n\n  if (!apiKey) {\n    throw new Error(\"OPENAI_API_KEY key is not set in params. params: {apiKey: 'sk-xxx'}\");\n  }\n\n  if (userPrompt) {\n    messagesCopy.push({\n      role: \"user\",\n      content: userPrompt,\n    });\n  }\n  if (images) {\n    const image_url =\n      params.model === \"gpt-4-vision-preview\"\n        ? images[0]\n        : {\n            url: images[0],\n            detail: \"high\",\n          };\n    messagesCopy.push({\n      role: \"user\",\n      content: [\n        {\n          type: \"image_url\",\n          image_url,\n        } as OpenAI.ChatCompletionContentPart,\n      ],\n    });\n  }\n\n  if (verbose) {\n    console.log(messagesCopy);\n  }\n\n  const chatParams = {\n    model: params.model || \"gpt-4o\",\n    messages: messagesCopy as unknown as OpenAI.ChatCompletionMessageParam[],\n    tools,\n    tool_choice,\n    max_tokens,\n    temperature: temperature ?? 0.7,\n    stream: !!stream,\n    response_format,\n  };\n\n  const urlPrefix = baseURL ?? \"https://api.openai.com/v1\";\n  const response = await fetch(urlPrefix + \"/chat/completions\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      Authorization: `Bearer ${apiKey}`,\n    },\n    body: JSON.stringify(chatParams),\n  });\n\n  if (!stream) {\n    if (response.status === 200) {\n      const result = await response.json();\n      return convertOpenAIChatCompletion(result, messagesCopy);\n    }\n    throw new Error(\"OPENAI API Error\");\n  }\n\n  // streaming\n  const reader = response.body?.getReader();\n\n  if (response.status !== 200 || !reader) {\n    throw new Error(\"Request failed\");\n  }\n\n  const decoder = new TextDecoder(\"utf-8\");\n  let done = false;\n  const buffer = [];\n  let text_buffer = \"\";\n  while (!done) {\n    const { done: readDone, value } = await reader.read();\n    if (readDone) {\n      done = readDone;\n      reader.releaseLock();\n    } else {\n      const text = decoder.decode(value, { stream: true });\n      text_buffer = text + text_buffer;\n      const lines = text_buffer.split(/\\n+/);\n      const next_buff = [];\n      for (const line of lines) {\n        try {\n          const json_text = line.replace(/^data:\\s*/, \"\");\n          if (json_text === \"[DONE]\") {\n            break;\n          } else if (json_text) {\n            const data = JSON.parse(json_text);\n            const token = data.choices[0].delta.content;\n            if (token) {\n              buffer.push(token);\n              if (filterParams && filterParams.streamTokenCallback && token) {\n                filterParams.streamTokenCallback(token);\n              }\n            }\n          }\n        } catch (__error) {\n          next_buff.push(line);\n        }\n      }\n      text_buffer = next_buff.join(\"\\n\");\n    }\n  }\n  return convertOpenAIChatCompletion(\n    {\n      choices: [\n        {\n          message: {\n            role: \"assistant\",\n            content: buffer.join(\"\"),\n            refusal: \"\",\n          },\n        },\n      ],\n    } as any,\n    messagesCopy,\n  );\n};\n\nconst input_sample = \"this is response result\";\nconst result_sample = {\n  object: \"chat.completion\",\n  id: \"chatcmpl-9N7HxXYbwjmdbdiQE94MHoVluQhyt\",\n  choices: [\n    {\n      message: {\n        role: \"assistant\",\n        content: input_sample,\n      },\n      finish_reason: \"stop\",\n      index: 0,\n      logprobs: null,\n    },\n  ],\n  created: 1715296589,\n  model: \"gpt-3.5-turbo-0125\",\n};\n\nexport const openAIMockAgent: AgentFunction<\n  {\n    model?: string;\n    query?: string;\n    system?: string;\n    verbose?: boolean;\n    temperature?: number;\n  },\n  Record<string, any> | string,\n  string | Array<any>\n> = async ({ filterParams }) => {\n  for await (const token of input_sample.split(\"\")) {\n    if (filterParams && filterParams.streamTokenCallback && token) {\n      await sleep(100);\n      filterParams.streamTokenCallback(token);\n    }\n  }\n\n  return result_sample;\n};\nconst openAIFetchAgentInfo: AgentFunctionInfo = {\n  name: \"openAIFetchAgent\",\n  agent: openAIFetchAgent,\n  mock: openAIMockAgent,\n  inputs: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: {\n        anyOf: [{ type: \"array\" }, { type: \"object\" }],\n      },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      baseURL: { type: \"string\" },\n      apiKey: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }],\n      },\n      stream: { type: \"boolean\" },\n      prompt: {\n        type: \"string\",\n        description: \"query string\",\n      },\n      messages: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }],\n        description: \"chat messages\",\n      },\n    },\n  },\n  output: {\n    type: \"object\",\n    properties: {\n      id: {\n        type: \"string\",\n      },\n      object: {\n        type: \"string\",\n      },\n      created: {\n        type: \"integer\",\n      },\n      model: {\n        type: \"string\",\n      },\n      choices: {\n        type: \"array\",\n        items: [\n          {\n            type: \"object\",\n            properties: {\n              index: {\n                type: \"integer\",\n              },\n              message: {\n                type: \"array\",\n                items: [\n                  {\n                    type: \"object\",\n                    properties: {\n                      content: {\n                        type: \"string\",\n                      },\n                      role: {\n                        type: \"string\",\n                      },\n                    },\n                    required: [\"content\", \"role\"],\n                  },\n                ],\n              },\n            },\n            required: [\"index\", \"message\", \"logprobs\", \"finish_reason\"],\n          },\n        ],\n      },\n      usage: {\n        type: \"object\",\n        properties: {\n          prompt_tokens: {\n            type: \"integer\",\n          },\n          completion_tokens: {\n            type: \"integer\",\n          },\n          total_tokens: {\n            type: \"integer\",\n          },\n        },\n        required: [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"],\n      },\n      text: {\n        type: \"string\",\n      },\n      tool: {\n        arguments: {\n          type: \"object\",\n        },\n        name: {\n          type: \"string\",\n        },\n      },\n      message: {\n        type: \"object\",\n        properties: {\n          content: {\n            type: \"string\",\n          },\n          role: {\n            type: \"string\",\n          },\n        },\n        required: [\"content\", \"role\"],\n      },\n    },\n    required: [\"id\", \"object\", \"created\", \"model\", \"choices\", \"usage\"],\n  },\n  params: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: { anyOf: [{ type: \"array\" }, { type: \"object\" }] },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      baseURL: { type: \"string\" },\n      apiKey: { anyOf: [{ type: \"string\" }, { type: \"object\" }] },\n      stream: { type: \"boolean\" },\n      prompt: { type: \"string\", description: \"query string\" },\n      messages: { anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }], description: \"chat messages\" },\n    },\n  },\n  outputFormat: {\n    llmResponse: {\n      key: \"choices.$0.message.content\",\n      type: \"string\",\n    },\n  },\n  samples: [\n    {\n      inputs: { prompt: input_sample },\n      params: {},\n      result: result_sample,\n    },\n  ],\n  description: \"OpenAI Fetch Agent\",\n  category: [\"llm\"],\n  author: \"Receptron team\",\n  repository: \"https://github.com/receptron/graphai\",\n  source: \"https://github.com/receptron/graphai/blob/main/llm_agents/openai_fetch_agent/src/openai_fetch_agent.ts\",\n  package: \"@graphai/openai_fetch_agent\",\n  license: \"MIT\",\n  stream: true,\n  npms: [\"openai\"],\n};\n\nexport default openAIFetchAgentInfo;\n"],"names":["Object","defineProperty","exports","value","llmMetaDataFirstTokenTime","llmMetaDataEndTime","convertMeta","getMessages","flatString","input","Array","isArray","filter","a","join","getMergeValue","namedInputs","params","key","values","inputValue","paramsValue","systemPrompt","messages","role","content","llmMetaData","start","firstToken","end","timing","latencyToFirstToken","undefined","duration","totalElapsed","Date","toISOString","initLLMMetaData","now","convertOpenAIChatCompletion","response","message","choices","text","functionResponse","tool_calls","tool","id","name","function","arguments","JSON","parse","__e","push","input_sample","result_sample","object","finish_reason","index","logprobs","created","model","openAIFetchAgentInfo","agent","async","filterParams","config","verbose","system","images","temperature","tools","tool_choice","max_tokens","prompt","response_format","apiKey","stream","baseURL","userPrompt","messagesCopy","Error","image_url","url","detail","type","console","log","chatParams","urlPrefix","fetch","method","headers","Authorization","body","stringify","status","result","json","reader","getReader","decoder","TextDecoder","done","buffer","text_buffer","readDone","read","releaseLock","decode","lines","split","next_buff","line","json_text","replace","token","delta","streamTokenCallback","__error","refusal","mock","sleep","inputs","properties","anyOf","description","output","items","required","usage","prompt_tokens","completion_tokens","total_tokens","outputFormat","llmResponse","samples","category","author","repository","source","package","license","npms"],"mappings":"wEACAA,OAAOC,eAAcC,EAAU,aAAc,CAAEC,OAAO,IACtDD,EAAAE,0BAAoCF,EAAAG,mBAA6BH,kBAA0BA,EAAAI,YAAsBJ,EAAAK,YAAsBL,gBAAwBA,EAAAM,gBAAqB,EAIpLN,EAAAM,WAHoBC,GACTC,MAAMC,QAAQF,GAASA,EAAMG,QAAQC,GAAMA,IAAGC,KAAK,MAASL,GAAS,GAQhFP,EAAAa,cALsB,CAACC,EAAaC,EAAQC,EAAKC,KAC7C,MAAMC,EAAaJ,EAAYE,GACzBG,EAAcJ,EAAOC,GAC3B,OAAOE,GAAcC,EAAc,EAAC,EAAInB,EAAQM,YAAYY,IAAa,EAAIlB,EAAQM,YAAYa,IAAcT,QAAQC,GAAMA,IAAGC,KAAK,OAAQ,EAAIZ,EAAQM,YAAYW,EAAO,EAOhLjB,EAAAK,YAJoB,CAACe,EAAcC,IACV,IAAKD,EAAe,CAAC,CAAEE,KAAM,SAAUC,QAASH,IAAkB,MAASC,GAAY,IAqBhHrB,EAAAI,YAhBqBoB,IACjB,MAAMC,MAAEA,EAAKC,WAAEA,EAAUC,IAAEA,GAAQH,EAAYI,OACzCC,EAAsBH,EAAaA,EAAaD,OAAQK,EACxDC,EAAWL,EAAaC,EAAMD,OAAaI,EAC3CE,EAAeL,EAAMF,EAC3B,MAAO,CACHG,OAAQ,CACJH,MAAO,IAAIQ,KAAKR,GAAOS,cACvBR,WAAYA,EAAa,IAAIO,KAAKP,GAAYQ,mBAAgBJ,EAC9DH,IAAK,IAAIM,KAAKN,GAAKO,cACnBL,sBACAE,WACAC,gBAEP,EAOLhC,EAAAmC,gBAJwB,KACA,CAAEP,OAAQ,CAAEH,MAAOQ,KAAKG,MAAOT,IAAK,EAAGK,aAAc,KAO7EhC,EAAAG,mBAH4BqB,IACxBA,EAAYI,OAAOD,IAAMM,KAAKG,KAAK,EAQvCpC,EAAAE,0BALmCsB,SACOM,IAAlCN,EAAYI,OAAOF,aACnBF,EAAYI,OAAOF,WAAaO,KAAKG,gBCvB7C,MAAMC,EAA8B,CAACC,EAAiCjB,KACpE,MAAMkB,EAAUD,GAAUE,QAAQ,IAAMF,GAAUE,QAAQ,GAAGD,QAAUD,GAAUE,QAAQ,GAAGD,QAAU,KAChGE,EAAOF,GAAWA,EAAQhB,QAAUgB,EAAQhB,QAAU,KAEtDmB,EAAmBH,GAASI,YAAcJ,GAASI,WAAW,GAAKJ,GAASI,WAAW,GAAK,KAG5FC,EAAOF,EACT,CACEG,GAAIH,EAAiBG,GACrBC,KAAMJ,GAAkBK,UAAUD,KAClCE,UAAW,MACT,IACE,OAAOC,KAAKC,MAAMR,GAAkBK,UAAUC,WAC9C,MAAOG,GACP,OAEH,EANU,SAQbrB,EAKJ,OAHIS,GACFlB,EAAS+B,KAAKb,GAET,IACFD,EACHG,OACAG,OACAL,UACAlB,WACD,EAiJGgC,EAAe,0BACfC,EAAgB,CACpBC,OAAQ,kBACRV,GAAI,yCACJL,QAAS,CACP,CACED,QAAS,CACPjB,KAAM,YACNC,QAAS8B,GAEXG,cAAe,OACfC,MAAO,EACPC,SAAU,OAGdC,QAAS,WACTC,MAAO,sBAuBHC,EAA0C,CAC9Cf,KAAM,mBACNgB,MAvLqHC,OACrHC,eACAjD,SACAD,cACAmD,aAEA,MAAMC,QAAEA,EAAOC,OAAEA,EAAMC,OAAEA,EAAMC,YAAEA,EAAWC,MAAEA,EAAKC,YAAEA,EAAWC,WAAEA,EAAUC,OAAEA,EAAMpD,SAAEA,EAAQqD,gBAAEA,GAAoB,IAC/G3D,KACAD,IAGC6D,OAAEA,EAAMC,OAAEA,EAAMC,QAAEA,GAAY,IAC/B9D,KACCkD,GAAU,CAAA,GAGVa,EAAajE,EAAAA,cAAcC,EAAaC,EAAQ,mBAAoB0D,GACpErD,EAAeP,EAAAA,cAAcC,EAAaC,EAAQ,kBAAmBoD,GAErEY,EAAe1E,EAAAA,YAA+Ce,EAAcC,GAElF,IAAKsD,EACH,MAAM,IAAIK,MAAM,uEASlB,GANIF,GACFC,EAAa3B,KAAK,CAChB9B,KAAM,OACNC,QAASuD,IAGTV,EAAQ,CACV,MAAMa,EACa,yBAAjBlE,EAAO6C,MACHQ,EAAO,GACP,CACEc,IAAKd,EAAO,GACZe,OAAQ,QAEhBJ,EAAa3B,KAAK,CAChB9B,KAAM,OACNC,QAAS,CACP,CACE6D,KAAM,YACNH,gBAMJf,GACFmB,QAAQC,IAAIP,GAGd,MAAMQ,EAAa,CACjB3B,MAAO7C,EAAO6C,OAAS,SACvBvC,SAAU0D,EACVT,QACAC,cACAC,aACAH,YAAaA,GAAe,GAC5BO,SAAUA,EACVF,mBAGIc,EAAYX,GAAW,4BACvBvC,QAAiBmD,MAAMD,EAAY,oBAAqB,CAC5DE,OAAQ,OACRC,QAAS,CACP,eAAgB,mBAChBC,cAAe,UAAUjB,KAE3BkB,KAAM5C,KAAK6C,UAAUP,KAGvB,IAAKX,EAAQ,CACX,GAAwB,MAApBtC,EAASyD,OAAgB,CAC3B,MAAMC,QAAe1D,EAAS2D,OAC9B,OAAO5D,EAA4B2D,EAAQjB,GAE7C,MAAM,IAAIC,MAAM,oBAIlB,MAAMkB,EAAS5D,EAASuD,MAAMM,YAE9B,GAAwB,MAApB7D,EAASyD,SAAmBG,EAC9B,MAAM,IAAIlB,MAAM,kBAGlB,MAAMoB,EAAU,IAAIC,YAAY,SAChC,IAAIC,GAAO,EACX,MAAMC,EAAS,GACf,IAAIC,EAAc,GAClB,MAAQF,GAAM,CACZ,MAAQA,KAAMG,EAAQxG,MAAEA,SAAgBiG,EAAOQ,OAC/C,GAAID,EACFH,EAAOG,EACPP,EAAOS,kBACF,CAELH,EADaJ,EAAQQ,OAAO3G,EAAO,CAAE2E,QAAQ,IACxB4B,EACrB,MAAMK,EAAQL,EAAYM,MAAM,OAC1BC,EAAY,GAClB,IAAK,MAAMC,KAAQH,EACjB,IACE,MAAMI,EAAYD,EAAKE,QAAQ,YAAa,IAC5C,GAAkB,WAAdD,EACF,MACK,GAAIA,EAAW,CACpB,MACME,EADOlE,KAAKC,MAAM+D,GACLzE,QAAQ,GAAG4E,MAAM7F,QAChC4F,IACFZ,EAAOnD,KAAK+D,GACRnD,GAAgBA,EAAaqD,qBAAuBF,GACtDnD,EAAaqD,oBAAoBF,KAIvC,MAAOG,GACPP,EAAU3D,KAAK4D,GAGnBR,EAAcO,EAAUnG,KAAK,OAGjC,OAAOyB,EACL,CACEG,QAAS,CACP,CACED,QAAS,CACPjB,KAAM,YACNC,QAASgF,EAAO3F,KAAK,IACrB2G,QAAS,OAKjBxC,EACD,EA6CDyC,KAbEzD,OAASC,mBACX,UAAW,MAAMmD,KAAS9D,EAAayD,MAAM,IACvC9C,GAAgBA,EAAaqD,qBAAuBF,UAChDM,EAAAA,MAAM,KACZzD,EAAaqD,oBAAoBF,IAIrC,OAAO7D,CAAa,EAMpBoE,OAAQ,CACNtC,KAAM,SACNuC,WAAY,CACV/D,MAAO,CAAEwB,KAAM,UACfjB,OAAQ,CAAEiB,KAAM,UAChBd,MAAO,CAAEc,KAAM,UACfb,YAAa,CACXqD,MAAO,CAAC,CAAExC,KAAM,SAAW,CAAEA,KAAM,YAErCZ,WAAY,CAAEY,KAAM,UACpBlB,QAAS,CAAEkB,KAAM,WACjBf,YAAa,CAAEe,KAAM,UACrBP,QAAS,CAAEO,KAAM,UACjBT,OAAQ,CACNiD,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,YAEtCR,OAAQ,CAAEQ,KAAM,WAChBX,OAAQ,CACNW,KAAM,SACNyC,YAAa,gBAEfxG,SAAU,CACRuG,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UACxDyC,YAAa,mBAInBC,OAAQ,CACN1C,KAAM,SACNuC,WAAY,CACV9E,GAAI,CACFuC,KAAM,UAER7B,OAAQ,CACN6B,KAAM,UAERzB,QAAS,CACPyB,KAAM,WAERxB,MAAO,CACLwB,KAAM,UAER5C,QAAS,CACP4C,KAAM,QACN2C,MAAO,CACL,CACE3C,KAAM,SACNuC,WAAY,CACVlE,MAAO,CACL2B,KAAM,WAER7C,QAAS,CACP6C,KAAM,QACN2C,MAAO,CACL,CACE3C,KAAM,SACNuC,WAAY,CACVpG,QAAS,CACP6D,KAAM,UAER9D,KAAM,CACJ8D,KAAM,WAGV4C,SAAU,CAAC,UAAW,YAK9BA,SAAU,CAAC,QAAS,UAAW,WAAY,oBAIjDC,MAAO,CACL7C,KAAM,SACNuC,WAAY,CACVO,cAAe,CACb9C,KAAM,WAER+C,kBAAmB,CACjB/C,KAAM,WAERgD,aAAc,CACZhD,KAAM,YAGV4C,SAAU,CAAC,gBAAiB,oBAAqB,iBAEnDvF,KAAM,CACJ2C,KAAM,UAERxC,KAAM,CACJI,UAAW,CACToC,KAAM,UAERtC,KAAM,CACJsC,KAAM,WAGV7C,QAAS,CACP6C,KAAM,SACNuC,WAAY,CACVpG,QAAS,CACP6D,KAAM,UAER9D,KAAM,CACJ8D,KAAM,WAGV4C,SAAU,CAAC,UAAW,UAG1BA,SAAU,CAAC,KAAM,SAAU,UAAW,QAAS,UAAW,UAE5DjH,OAAQ,CACNqE,KAAM,SACNuC,WAAY,CACV/D,MAAO,CAAEwB,KAAM,UACfjB,OAAQ,CAAEiB,KAAM,UAChBd,MAAO,CAAEc,KAAM,UACfb,YAAa,CAAEqD,MAAO,CAAC,CAAExC,KAAM,SAAW,CAAEA,KAAM,YAClDZ,WAAY,CAAEY,KAAM,UACpBlB,QAAS,CAAEkB,KAAM,WACjBf,YAAa,CAAEe,KAAM,UACrBP,QAAS,CAAEO,KAAM,UACjBT,OAAQ,CAAEiD,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,YAC9CR,OAAQ,CAAEQ,KAAM,WAChBX,OAAQ,CAAEW,KAAM,SAAUyC,YAAa,gBACvCxG,SAAU,CAAEuG,MAAO,CAAC,CAAExC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UAAYyC,YAAa,mBAGjGQ,aAAc,CACZC,YAAa,CACXtH,IAAK,6BACLoE,KAAM,WAGVmD,QAAS,CACP,CACEb,OAAQ,CAAEjD,OAAQpB,GAClBtC,OAAQ,CAAA,EACRiF,OAAQ1C,IAGZuE,YAAa,qBACbW,SAAU,CAAC,OACXC,OAAQ,iBACRC,WAAY,uCACZC,OAAQ,yGACRC,QAAS,8BACTC,QAAS,MACTjE,QAAQ,EACRkE,KAAM,CAAC"}